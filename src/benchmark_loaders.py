"""
Benchmark Loaders for Self-Healing Code Agent

This module provides loaders for various coding benchmarks that can be used
to evaluate the self-healing agent. Each loader transforms benchmark-specific
data formats into a standardized format compatible with the agent workflow.

Supported Benchmarks:
- HumanEval (OpenAI): Function completion with unit tests
"""

import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Iterator
import json

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkProblem:
    """Standardized problem format for all benchmarks."""
    task_id: str                    # Unique identifier for the problem
    prompt: str                     # The prompt/specification for code generation
    test_code: str                  # Unit tests to validate the solution
    entry_point: str                # Function/class name that needs to be implemented
    canonical_solution: Optional[str] = None  # Reference solution (if available)

    def get_execution_code(self, generated_code: str) -> str:
        """
        Combine generated code with test code for execution.

        Args:
            generated_code: The code generated by the agent

        Returns:
            Complete executable code with tests
        """
        return f"{generated_code}\n\n{self.test_code}"


class BenchmarkLoader(ABC):
    """Abstract base class for benchmark loaders."""

    @property
    @abstractmethod
    def name(self) -> str:
        """Return the benchmark name."""
        pass

    @abstractmethod
    def load(self, path: str) -> List[BenchmarkProblem]:
        """
        Load all problems from the benchmark.

        Args:
            path: Path to the benchmark data file

        Returns:
            List of BenchmarkProblem objects
        """
        pass

    @abstractmethod
    def load_from_hub(self, split: str = "test") -> List[BenchmarkProblem]:
        """
        Load problems directly from HuggingFace Hub.

        Args:
            split: Dataset split to load (e.g., "test", "train")

        Returns:
            List of BenchmarkProblem objects
        """
        pass

    def iterate(self, path: str) -> Iterator[BenchmarkProblem]:
        """
        Iterate over problems one at a time (memory-efficient).

        Args:
            path: Path to the benchmark data file

        Yields:
            BenchmarkProblem objects one at a time
        """
        for problem in self.load(path):
            yield problem


class HumanEvalLoader(BenchmarkLoader):
    """
    Loader for OpenAI HumanEval benchmark.

    HumanEval format:
    - task_id: "HumanEval/0", "HumanEval/1", etc.
    - prompt: Function signature with docstring
    - canonical_solution: Reference implementation
    - test: Unit tests using check() function and assertions
    - entry_point: Function name to implement

    Dataset: https://huggingface.co/datasets/openai/openai_humaneval
    """

    @property
    def name(self) -> str:
        return "HumanEval"

    def load(self, path: str) -> List[BenchmarkProblem]:
        """
        Load HumanEval problems from a JSONL file.

        Args:
            path: Path to HumanEval JSONL file

        Returns:
            List of BenchmarkProblem objects
        """
        problems = []
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    problem = self._convert_to_problem(data)
                    problems.append(problem)
        return problems

    def load_from_hub(self, split: str = "test") -> List[BenchmarkProblem]:
        """
        Load HumanEval directly from HuggingFace Hub.

        Args:
            split: Dataset split (HumanEval only has "test")

        Returns:
            List of BenchmarkProblem objects
        """
        logger.info(f"HumanEvalLoader.load_from_hub() called with split={split}")

        try:
            from datasets import load_dataset
            logger.debug("datasets library imported successfully")
        except ImportError as e:
            logger.error(f"Failed to import datasets library: {e}")
            raise ImportError(
                "Please install the 'datasets' library: pip install datasets"
            )

        logger.info("Loading dataset from HuggingFace Hub: openai/openai_humaneval")
        try:
            dataset = load_dataset("openai/openai_humaneval", split=split)
            logger.info(f"Dataset loaded successfully. Size: {len(dataset)}")
        except Exception as e:
            logger.error(f"Failed to load dataset from Hub: {e}")
            raise

        problems = []
        logger.debug("Converting dataset items to BenchmarkProblem objects...")
        for i, item in enumerate(dataset):
            problem = self._convert_to_problem(item)
            problems.append(problem)
            if (i + 1) % 50 == 0:
                logger.debug(f"Converted {i + 1} problems...")

        logger.info(f"Conversion complete. Total problems: {len(problems)}")
        return problems

    def _convert_to_problem(self, data: dict) -> BenchmarkProblem:
        """
        Convert HumanEval data format to BenchmarkProblem.

        Args:
            data: Raw HumanEval problem data

        Returns:
            Standardized BenchmarkProblem
        """
        # The prompt includes function signature and docstring
        # The test includes the check function that validates the solution

        # Build test code that calls the entry point
        entry_point = data["entry_point"]
        test_code = data["test"]

        # HumanEval tests use a check() function pattern
        # We need to add the check() call at the end
        full_test_code = f"""{test_code}

check({entry_point})
"""

        return BenchmarkProblem(
            task_id=data["task_id"],
            prompt=data["prompt"],
            test_code=full_test_code,
            entry_point=entry_point,
            canonical_solution=data.get("canonical_solution")
        )


class MBPPLoader(BenchmarkLoader):
    """
    Loader for MBPP (Mostly Basic Python Problems) benchmark.

    MBPP format:
    - task_id: Integer ID
    - text: Natural language description
    - code: Reference solution
    - test_list: List of assert statements

    Dataset: https://huggingface.co/datasets/google-research-datasets/mbpp
    """

    @property
    def name(self) -> str:
        return "MBPP"

    def load(self, path: str) -> List[BenchmarkProblem]:
        """
        Load MBPP problems from a JSONL file.

        Args:
            path: Path to MBPP JSONL file

        Returns:
            List of BenchmarkProblem objects
        """
        problems = []
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data = json.loads(line)
                    problem = self._convert_to_problem(data)
                    problems.append(problem)
        return problems

    def load_from_hub(self, split: str = "test") -> List[BenchmarkProblem]:
        """
        Load MBPP directly from HuggingFace Hub.

        Args:
            split: Dataset split ("train", "test", "validation")

        Returns:
            List of BenchmarkProblem objects
        """
        try:
            from datasets import load_dataset
        except ImportError:
            raise ImportError(
                "Please install the 'datasets' library: pip install datasets"
            )

        dataset = load_dataset("google-research-datasets/mbpp", split=split)
        problems = []
        for item in dataset:
            problem = self._convert_to_problem(item)
            problems.append(problem)
        return problems

    def _convert_to_problem(self, data: dict) -> BenchmarkProblem:
        """
        Convert MBPP data format to BenchmarkProblem.
        """
        # Extract function name from the code (first def statement)
        code = data.get("code", "")
        entry_point = self._extract_function_name(code)

        # Combine test assertions
        test_list = data.get("test_list", [])
        test_code = "\n".join(test_list)

        return BenchmarkProblem(
            task_id=f"MBPP/{data['task_id']}",
            prompt=data["text"],
            test_code=test_code,
            entry_point=entry_point,
            canonical_solution=code
        )

    def _extract_function_name(self, code: str) -> str:
        """Extract the main function name from code."""
        import re
        match = re.search(r'def\s+(\w+)\s*\(', code)
        return match.group(1) if match else "solution"


# Registry of available benchmark loaders
BENCHMARK_LOADERS = {
    "humaneval": HumanEvalLoader,
    "mbpp": MBPPLoader,
}


def get_loader(benchmark_name: str) -> BenchmarkLoader:
    """
    Get a benchmark loader by name.

    Args:
        benchmark_name: Name of the benchmark (case-insensitive)

    Returns:
        Instantiated benchmark loader

    Raises:
        ValueError: If benchmark is not supported
    """
    logger.info(f"get_loader() called with benchmark_name={benchmark_name}")
    name_lower = benchmark_name.lower()
    if name_lower not in BENCHMARK_LOADERS:
        available = ", ".join(BENCHMARK_LOADERS.keys())
        logger.error(f"Unknown benchmark: {benchmark_name}. Available: {available}")
        raise ValueError(
            f"Unknown benchmark: {benchmark_name}. Available: {available}"
        )
    loader = BENCHMARK_LOADERS[name_lower]()
    logger.info(f"Created loader: {loader.name}")
    return loader


def list_benchmarks() -> List[str]:
    """Return list of supported benchmark names."""
    return list(BENCHMARK_LOADERS.keys())
